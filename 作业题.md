# 作业题

*自动化钱  71 吴思源*

---

#### 1.为什么sigmoid函数和tanh函数会导致梯度消失？有什么解决方案？

#### 梯度消失现象的原因

通常使用的激活函数是sigmoid函数和tanh，绘制出函数导数图像如下

![图1 sigmoid函数曲线](/home/edmund/桌面/Figure_4.png)



通过这两个函数导数的图像，我们可以看出sigmoid函数导数不超过0.25，tanh函数导数不超过1，而且在x = 0 处有最大的导数。

对于深度神经网络，优化神经网络的方法一般是使用反向传播算法更新梯度，但是这就涉及到了多次偏导数的计算，根据链式求导法则，最后的更新将以激活函数导数值指数形式变化。

对于sigmoid函数和tanh函数，其导数值小于1,因此层数增加，梯度更新值以函数偏导数值指数形式衰减，当层数很大时候，每次梯度更新值会很小，这就引发了梯度消失现象。梯度消失（或者是梯度弥散），直接导致了浅层网络权值没办法更新（梯度传不到浅层网络）。如果激活函数是导数值大于1的函数，则会出现梯度爆炸的现象。

#### 解决方法

1. 使用ReLU、Leaky ReLU、eLU作为激活函数
2. 预训练+微调：每训练一层隐节点，就把上一层隐节点的输出作为这一层的输入，把本层的输出作为下一层的输入，进行逐层预训练。在预训练结束后再对整个网络进行训练，这时候只需要微调。
3. 使用DRN解决梯度消失问题：在网络中加入skip connection，根本上解决了梯度消失的问题；但是ResNet的网络结构设计对内存要求很高，如果使用DenseNet会降低计算量。



---

#### 2.ReLU系列的激活函数的优点是什么？有什么局限性，如何加以改进？



#### ReLU

ReLU（修正线性单元）激活函数为 
$$
ReLU(x)  = \max{(0,x)}
$$
优点有

1. 不容易出现梯度消失问题

2. 代码简单，计算量小

3. 收敛速度快

*缺点*：容易死掉（某些神经元参数不会被更新）；不是zero-centered

#### Leaky ReLU

函数式为
$$
LeakyReLU(x) = \max{(\alpha x,x)}
\space\space\space

\alpha=0.01
$$
对于Leaky ReLU，经验上$ \alpha$取0.01，但是也可以通过训练学出$\alpha$ 的值

优点：有ReLU所有的优点，而且比ReLU不容易死掉

也有资料显示这个改进不太好

#### Parametric ReLU

PReLU（参数化修正线性单元）

对于ReLU系函数，通过学习确定其负值部分的斜率
$$
PReLU(x) = \max{(0,x)}+\alpha \min{(0,x)}
$$
其中，$\alpha$是可学习参数，就是负值部分的斜率

#### Randomized ReLU

RReLU（随机纠正线性单元）

负值部分在训练中是随机的，但是在测试时变成固定的



---

#### 3.写出多层感知机的平方误差损失函数和交叉熵损失函数

1. *平方误差损失函数*

对于单个样本有n个输出的损失函数为：
$$
E(j) = \frac{1}{2}\sum_{i=1}^n {(\hat y_k^{(i)}-y_k^{(i)})^2}
$$
对于训练集上m个样本的平方误差损失函数为


$$
E = \sum_{j=1}^m {E(j)}= \frac{1}{2m}\sum_{j=1}^m\sum_{i=1}^n {(\hat y_k^{(i)}-y_k^{(i)})^2}
$$


2.*交叉熵损失函数*
$$
E = \sum_{i=1}^n{(-y\log(\hat y_k^{(i)})-(1-y)\log(1-\hat y_k^{(i)}))}
$$


#### 4. 平方误差损失函数和交叉熵损失函数分别适用于什么场景？





#### 5.模型的方差和偏差的关系？





#### 6.如何解决模型的过拟合和欠拟合问题？

- 防止过拟合的方法

  - 早停（early-stopping）

    将数据分成训练集和验证集，训练集用来计算和更新梯度、连接权、阈值，使用验证集来确定误差，如果训练集误差减小而验证集误差上升，则出现过拟合，此时停止训练，返回最小验证集误差的连接权和阈值矩阵

  - 正则化(regularization)

    在损失函数中增加描述网络复杂程度的正则项，由正则项和原损失函数加权确定损失函数，通过调整权重防止进入过拟合

- 防止欠拟合的方法




#### 7.卷积神经网络相对于多层感知机的优点有哪些？







----

----



### 编程题

#### 1.实现卷积运算

```python
#!usr/bin/env python

import numpy as np

input_data=[
            [10,10,10,0,0,0],
            [10,10,10,0,0,0],
            [10,10,10,0,0,0],
            [0,0,0,10,10,10],
            [0,0,0,10,10,10],
            [0,0,0,10,10,10]
            ]
weights_data=[ 
               [ 1, 1, 1],
               [0, 0, 0],
               [-1,-1,-1],
           	]



def my_conv(A,B):
    '''
    convolution
    Argument:
    A -- Matrix
    B -- filter
    
    Return:
    output -- matrix

    
    '''
    A = np.array(A)
    B = np.array(B)
    [i_A,j_A] = np.shape(A)
    #m = B.shape()
    [i_B,j_B] = np.shape(B)
    output = np.ones([i_A-i_B+1,j_A-j_B+1])
    i_out = i_A-i_B+1
    j_out = j_A-j_B+1
    print(i_out,j_out)
    for i_index in range(0,i_out):
        for j_index in range(0,j_out):
            Tem = np.zeros([i_B,j_B])
            Tem = A[i_index:i_index+i_B,j_index:j_index+j_B]*B
            output[i_index,j_index] = sum(sum(Tem))
    return(output)


output_data = my_conv(input_data,weights_data)

print(output_data)

```





#### 2.编程实现sigmoid和tanh

```python
#!usr/bin/env python

import numpy as np 

def my_sigmoid(x):
    ''' 
    This is sigmoid function

    Argument:
    x -- a matrix
    Return:
    z -- the value of sigmoid function 
    '''
    z = 1/(1+np.exp(-x))
    return z

def my_tanh(x):
    '''
    This is the tanh function
   
   	Argument:
    x -- a matrix
    Return:
    z -- value of tanh function
    '''
    z = (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))
    return z

## 验证
A = np.array([[1,2,3,4,5],
            [6,7,8,9,10]])

print(my_sigmoid(A))
print(np.tanh(A))
print(my_tanh(A))

```

